from fastapi import APIRouter, Form
from fastapi.responses import JSONResponse
from modules.llm import get_llm_chain
from modules.query_handlers import query_chain
from langchain_core.documents import Document
from langchain.schema import BaseRetriever
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pinecone import Pinecone
from pydantic import Field
from typing import List, Optional
from logger import logger
import os

router=APIRouter()

@router.post("/ask/")
async def ask_question(question: str = Form(...)):
    try:
        logger.info(f"user query: {question}")

        # Embed model + Pinecone setup
        pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
        index = pc.Index(os.environ["PINECONE_INDEX_NAME"])
        embed_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        embedded_query = embed_model.embed_query(question)
        res = index.query(vector=embedded_query, top_k=3, include_metadata=True)

        docs = [
            Document(
                page_content=match["metadata"].get("text", ""),
                metadata=match["metadata"]
            ) for match in res["matches"]
        ]

        class SimpleRetriever(BaseRetriever):
            tags: Optional[List[str]] = Field(default_factory=list)
            metadata: Optional[dict] = Field(default_factory=dict)

            def __init__(self, documents: List[Document]):
                super().__init__()
                self._docs = documents

            def _get_relevant_documents(self, query: str) -> List[Document]:
                return self._docs

        # Fallback: If no docs, answer using LLM only
        if not docs:
            from langchain_groq import ChatGroq
            from langchain.prompts import PromptTemplate

            llm = ChatGroq(groq_api_key=os.environ["GROQ_API_KEY"], model_name="mixtral-8x7b-32768")
            prompt = PromptTemplate(
                input_variables=["question"],
                template="""
You are MediBot, an AI-powered assistant for medical questions. Answer the user's question using your general medical knowledge.

üôã‚Äç‚ôÇÔ∏è **User Question**:
{question}

üí¨ **Answer**:
- Respond in a calm, factual, and respectful tone.
- Use simple explanations when needed.
- Do NOT make up facts.
- Do NOT give medical advice or diagnoses.
"""
            )
            answer = llm.invoke(prompt.format(question=question))
            logger.info("query successful (fallback)")
            return {"answer": answer}

        retriever = SimpleRetriever(docs)
        chain = get_llm_chain(retriever)
        result = query_chain(chain, question)

        logger.info("query successful")
        return result

    except Exception as e:
        logger.exception("Error processing question")
        return JSONResponse(status_code=500, content={"error": str(e)})